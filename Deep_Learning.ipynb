{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8c0073-5cb9-4689-bd5d-43b4983e3697",
   "metadata": {},
   "source": [
    "# Deep Learning Frameworks Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bdd6ca6-6905-4640-8ce7-ae476e93c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is TensorFlow 2.0, and how is it different from TensorFlow 1.x?\n",
    "#Ans. TensorFlow 2.0 (TF2) is a major update to the popular open-source machine learning library, TensorFlow, aiming for simpler execution, easier use,\n",
    "#     and improved developer productivity. Key differences from TensorFlow 1.x (TF1) include a focus on ease of use with Keras, eager execution by\n",
    "#     default, and a cleaned-up API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1091fe09-83e3-47ea-b4a2-ef4cb8751206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.  How do you install TensorFlow 2.0?\n",
    "#Ans. To install TensorFlow 2.0 for deep learning, you'll typically use pip (the Python package installer) and potentially a virtual environment. For\n",
    "#     GPU support, you may need to install CUDA and cuDNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b0db6c2-3497-415c-9c5c-f13791219e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  What is the primary function of the tf.function in TensorFlow 2.0?\n",
    "#Ans. In TensorFlow 2.0, tf.function primarily serves to convert Python functions into TensorFlow graphs, enabling graph execution and optimizing\n",
    "#     performance. This allows for faster and more efficient model deployment. In the context of deep learning, this is crucial for building and\n",
    "#     deploying models that can be used for inference and training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ffa9fa-fb44-4cb1-9393-3d7efd982676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.  What is the purpose of the Model class in TensorFlow 2.0?\n",
    "#Ans. In TensorFlow 2.0, the Model class is the foundation for defining and training neural networks, serving as the central class for building and \n",
    "#    managing models. It provides a structured way to define model architecture, compile the model, train it using methods like fit(), evaluate it, and\n",
    "#    make predictions using predict(). The Model class also facilitates saving and loading the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a40c0e88-a241-4335-b743-b7e58da67867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.  How do you create a neural network using TensorFlow 2.0?\n",
    "#Ans. To create a neural network using TensorFlow 2.0, you'll typically follow these steps: define the model architecture using tf.keras.Sequential, add\n",
    "#     layers (like Dense, Conv2D, Flatten), compile the model (specifying loss function, optimizer, and metrics), and then train it using a dataset. \n",
    "#1. Setup and Import Libraries:\n",
    "#Install TensorFlow 2.0: Use pip install tensorflow.\n",
    "#Import necessary libraries: import tensorflow as tf. \n",
    "#2. Define the Model Architecture:\n",
    "#Sequential Model: Use tf.keras.Sequential to build a sequential model, where layers are added one after another. \n",
    "#Layers:\n",
    "#Dense Layer: For fully connected layers (each neuron in one layer connects to every neuron in the next layer). \n",
    "#Conv2D Layer: For convolutional neural networks (CNNs) used in image processing, applying filters to extract features. \n",
    "#Flatten Layer: Used to convert multi-dimensional data (like image pixels) into a 1D array.\n",
    "#3. Compile the Model:\n",
    "#Loss Function: Select a loss function (e.g., sparse_categorical_crossentropy for multi-class classification, binary_crossentropy for binary \n",
    "#classification).\n",
    "#Optimizer: Choose an optimizer (e.g., adam, sgd, rmsprop).\n",
    "#Metrics: Specify metrics to evaluate the model's performance (e.g., accuracy, f1_score).\n",
    "#4. Load and Prepare Data:\n",
    "#Datasets:\n",
    "#Use TensorFlow's tf.keras.datasets to load datasets (e.g., mnist, fashion_mnist, imdb). \n",
    "#Preprocessing:\n",
    "#Clean and prepare the data (e.g., normalize pixel values, one-hot encode labels). \n",
    "#5. Train the Model:\n",
    "#Fit Method: Use the model.fit() method to train the model on the training data.\n",
    "#Epochs: Specify the number of epochs (iterations over the entire dataset).\n",
    "#Batch Size: Set the batch size (number of samples processed in each iteration).\n",
    "#Validation Data: Optionally, use validation data to monitor the model's performance on unseen data during training.\n",
    "#6. Evaluate the Model:\n",
    "#Evaluate Method: Use the model.evaluate() method to assess the model's performance on the test data.\n",
    "#Metrics: Check the metrics (e.g., accuracy, loss) to see how well the model generalizes to new data.\n",
    "#7. Make Predictions:\n",
    "#Predict Method: Use the model.predict() method to make predictions on new, unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5587f92-8a88-41f5-be24-c272b80360ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.  What is the importance of Tensor Space in TensorFlow?\n",
    "#Ans. TensorSpace is a neural network 3D visualization framework built using TensorFlow.js, Three.js, and Tween.js. It provides Keras-like APIs to\n",
    "#     construct deep learning layers, load pre-trained models, and generate 3D visualizations within a browser. TensorSpace aims to offer an intuitive\n",
    "#     understanding of model structures, training processes, and prediction mechanisms through visual representations of intermediate information. \n",
    "#     TensorSpace supports pre-trained models from TensorFlow, Keras, and TensorFlow.js. To visualize these models, they must first be preprocessed using\n",
    "#     the TensorSpace Converter, which extracts information from hidden layers and prepares the model for visualization. This involves converting the \n",
    "#     original model, which typically only returns the final output, into a new model that exposes all desired intermediate outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15c3a806-c8fa-485b-a746-fadac2980ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.  How can TensorBoard be integrated with TensorFlow 2.0?\n",
    "#Ans. TensorBoard integrates seamlessly with TensorFlow 2.0, allowing users to visualize and analyze the training process of their deep learning models.\n",
    "#     It provides dashboards to track various metrics, view model graphs, and analyze data distributions. TensorBoard can be integrated directly within\n",
    "#     Jupyter notebooks or by using command-line tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd147cb8-7368-4e36-9562-ce8ba731bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.  What is the purpose of TensorFlow Playground?\n",
    "#Ans. TensorFlow Playground is a browser-based application designed to help users understand and experiment with neural networks in a simplified, \n",
    "#     interactive way. It allows users to visualize the impact of different hyperparameters and network configurations on model performance, making it\n",
    "#     an excellent educational and exploratory tool for deep learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0fa0cb4-2eab-4f29-809c-a6a6619ba2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.  What is Neutron, and how is it useful for deep learning models?\n",
    "#Ans. In a deep learning context, \"neutron\" refers to the individual processing units within a neural network, not the subatomic particle. These neurons,\n",
    "#     or nodes, are the fundamental building blocks of the network and are responsible for processing data and performing calculations. \n",
    "# How Neurons are Useful in Deep Learning\n",
    "#Learning Complex Patterns:\n",
    "#By connecting neurons in multiple layers, deep learning models can learn complex patterns and representations from data. \n",
    "#Non-Linear Transformations:\n",
    "#Neurons perform non-linear transformations on the input data, allowing the model to learn intricate relationships. \n",
    "#Feature Extraction:\n",
    "#Neurons in different layers can learn to extract different features from the data, enabling the model to learn from raw data and improve its performance. \n",
    "#Versatility:\n",
    "#Deep learning models using neurons are used in various applications like image recognition, natural language processing, and more. \n",
    "#Predictive Power:\n",
    "#The network can be trained to make predictions by adjusting the connections and weights between neurons, allowing it to generalize to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c2c5fe4-442f-4de5-959e-72185e377a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.  What is the difference between TensorFlow and PyTorch?\n",
    "#Ans. TensorFlow and PyTorch are both open-source deep learning frameworks, but they have key differences:\n",
    "#Computation graph:\n",
    "#TensorFlow uses a static computation graph, while PyTorch uses a dynamic computation graph. Static graphs are defined before execution, which can lead\n",
    "#to more efficient training for large models. Dynamic graphs, on the other hand, are built as the code is executed, offering more flexibility for \n",
    "#research and debugging.\n",
    "\n",
    "#Ease of use:\n",
    "#PyTorch is often considered more Pythonic and easier to learn, especially for beginners. TensorFlow has a steeper learning curve, though the high-level\n",
    "#API Keras, which runs on top of TensorFlow, simplifies model building.\n",
    "\n",
    "#Community and ecosystem:\n",
    "#TensorFlow has a larger, more established community, while PyTorch's community is rapidly growing, particularly in academia and research. Both\n",
    "#frameworks have extensive resources and libraries, but TensorFlow's maturity gives it an edge in production deployment.\n",
    "\n",
    "#Debugging:\n",
    "#PyTorch's dynamic graph makes debugging more straightforward, as errors are caught immediately. TensorFlow's static graph can make debugging more\n",
    "#challenging, though tools like TensorBoard aid in visualizing the training process.\n",
    "\n",
    "#Performance:\n",
    "#TensorFlow's static graph and optimized execution can lead to faster training and inference for large models and production deployments. PyTorch's\n",
    "#dynamic graph can be slower but offers flexibility for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e14fe22b-b415-4a2e-9dc2-412e72efabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.  How do you install PyTorch?\n",
    "#Ans. 1. Install Anaconda or Miniconda:\n",
    "#If you don't have it already, download and install Anaconda or Miniconda, a package management system. \n",
    "#2. Create a Conda Environment:\n",
    "#Use the conda create --name pytorch-env command to create a dedicated environment for PyTorch. \n",
    "#3. Activate the Environment:\n",
    "#Activate the newly created environment with conda activate pytorch-env. \n",
    "#4. Install PyTorch:\n",
    "#Using Conda: Go to the PyTorch installation website and copy the appropriate Conda installation command. The command might look something like conda\n",
    "#install pytorch torchvision torchaudio cpuonly -c pytorch (if you're using CPU only) or include CUDA if you have an Nvidia GPU. \n",
    "#Using Pip: If you're not using Conda, you can use pip. First, ensure you have a suitable Python environment. Then, install PyTorch, torchvision, and \n",
    "#torchaudio with commands like pip install torch torchvision torchaudio. \n",
    "#5. Verify Installation:\n",
    "#Open a Python shell within the Conda environment and try importing the torch module. If the installation is successful, you should be able to import it\n",
    "#without errors. You can also create a simple tensor to further verify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87732a2e-af8e-486f-a272-26c66d808328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.  What is the basic structure of a PyTorch neural network?\n",
    "#Ans. The basic structure of a PyTorch neural network involves several key components:\n",
    "\n",
    "#Tensors: Tensors are the fundamental data structure in PyTorch, similar to NumPy arrays, but with GPU acceleration capabilities. They are used to store\n",
    "#         and manipulate the network's inputs, outputs, and parameters. \n",
    "\n",
    "#Layers: Neural networks consist of layers that perform operations on the data. Common layer types include: \n",
    "#Linear Layers (nn.Linear): Perform a linear transformation on the input data.\n",
    "#Activation Functions: Introduce non-linearity to the network, allowing it to learn complex patterns (e.g., ReLU, sigmoid, tanh).\n",
    "#Convolutional Layers (nn.Conv2d): Extract features from image data.\n",
    "#Pooling Layers (nn.MaxPool2d): Reduce the spatial dimensions of feature maps.\n",
    "#Recurrent Layers (nn.RNN, nn.LSTM, nn.GRU): Process sequential data.\n",
    "#Modules (nn.Module): Modules are the building blocks of PyTorch neural networks. They encapsulate layers and other operations, and can be nested to\n",
    "\n",
    "#create complex architectures.\n",
    "#Model Definition: A neural network model is typically defined as a class that inherits from nn.Module. The __init__ method initializes the layers, and\n",
    "#                 the forward method defines the flow of data through the network.\n",
    "#Loss Function: A loss function measures the difference between the network's predictions and the true values. Common loss functions include mean squared\n",
    "#               error (MSE) and cross-entropy loss.\n",
    "#Optimizer: An optimizer updates the network's parameters during training to minimize the loss function. Common optimizers include stochastic gradient\n",
    "#           descent (SGD) and Adam.\n",
    "#Training Loop: The training loop involves iterating over the data, performing a forward pass, calculating the loss, performing backpropagation to\n",
    "#               compute gradients, and updating the parameters using the optimizer.\n",
    "#Input Layer: Data is fed into the network through the input layer. \n",
    "#Hidden Layers: The layers between the input and output layers are called hidden layers. \n",
    "#Output Layer: The output layer is used to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e2e81fa-933d-4dd6-9e93-8a86c5e97adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13.  What is the significance of tensors in PyTorch?\n",
    "#Ans. In PyTorch and deep learning, tensors are fundamental data structures used to represent and manipulate data. They are similar to NumPy arrays but\n",
    "#    with the added capability of running on both CPU and GPU, enabling efficient computation, especially for large datasets and complex models. \n",
    "# Tensors are significant because of the following reasons:\n",
    "#Data Storage and Representation:\n",
    "#Tensors store and represent various types of data in deep learning, including input data, model weights, biases, predictions, and intermediate \n",
    "#computations. \n",
    "#GPU Acceleration:\n",
    "#Tensors can be readily moved to and from GPUs, allowing for significantly faster computations during training and inference, crucial for large and\n",
    "#complex models. \n",
    "#Automatic Differentiation:\n",
    "#PyTorch's automatic differentiation system leverages tensors to track and compute gradients, enabling the optimization of neural network models. \n",
    "#Flexibility and Efficiency:\n",
    "#Tensors offer flexibility in terms of data manipulation, allowing for operations like reshaping, slicing, and contraction, which are essential for\n",
    "#various deep learning tasks. \n",
    "#Core of Neural Networks:\n",
    "#Tensors are the foundation for building and training neural networks, representing both the inputs and outputs of each layer and the parameters within\n",
    "# those layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ea2647-82e1-4533-8de8-41741a4ab0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14.  What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch?\n",
    "#Ans. In PyTorch, the core difference between torch.Tensor and torch.cuda.Tensor lies in their memory allocation and computational execution.\n",
    "# torch.Tensor: This represents a tensor object residing in the CPU's main memory. Operations performed on torch.Tensor are executed by the CPU.\n",
    "\n",
    "#torch.cuda.Tensor: This represents a tensor object residing in the GPU's memory. Operations performed on torch.cuda.Tensor are executed by the GPU,\n",
    "#leveraging its parallel processing capabilities for faster computation, especially beneficial in deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1fd9421-728a-443f-b8bb-03c1af72f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15.  What is the purpose of the torch.optim module in PyTorch?\n",
    "#Ans. The torch.optim module in PyTorch provides various optimization algorithms essential for training deep learning models. These algorithms adjust\n",
    "#     the model's parameters, such as weights and biases, to minimize a loss function. By iteratively updating the parameters based on computed gradients\n",
    "#     the optimizer guides the model toward better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a4893c-e06a-4448-aca9-592f80adc5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16.  What are some common activation functions used in neural networks?\n",
    "#Ans. In deep learning, common activation functions include Sigmoid, Tanh, ReLU, Leaky ReLU, ELU, and Softmax. These functions introduce non-linearity\n",
    "#     into neural networks, enabling them to learn complex relationships in data. \n",
    "\n",
    "#The detailed explaination of activation functions:\n",
    "#Sigmoid:\n",
    "#This function maps input values to the range, making it suitable for binary classification tasks. \n",
    "\n",
    "#Tanh:\n",
    "#A hyperbolic tangent function, Tanh maps input values to the range [-1, 1], potentially accelerating training by centering data around zero. \n",
    "\n",
    "#ReLU (Rectified Linear Unit):\n",
    "#A widely used function that returns the input if it's positive and zero otherwise. ReLU is computationally efficient and helps mitigate the vanishing \n",
    "#gradient problem in deep networks. \n",
    "\n",
    "#Leaky ReLU:\n",
    "#A variation of ReLU that allows small gradients for negative inputs, preventing \"dying neurons\". \n",
    "\n",
    "#ELU (Exponential Linear Unit):\n",
    "#A function that provides smooth gradient propagation and can accelerate learning, similar to Leaky ReLU, but with a more sophisticated approach to\n",
    "#negative values. \n",
    "\n",
    "#Softmax:\n",
    "#Primarily used for multi-class classification, Softmax converts a vector of raw outputs into a probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd702b8-a1c5-476f-8c6c-87f742efcc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17.  What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch?\n",
    "#Ans. torch.nn.Module and torch.nn.Sequential serve different purposes in defining neural networks in PyTorch. \n",
    "# torch.nn.Module is the base class for all neural network modules. When creating custom neural network architectures, one subclasses nn.Module and\n",
    "# defines the network's layers in the __init__ method and the forward pass logic in the forward method. This approach offers flexibility in designing\n",
    "# complex network structures with branching or skip connections.\n",
    "# torch.nn.Sequential is a container that allows stacking modules in a sequential order. It simplifies the creation of linear stacks of layers where the\n",
    "# output of one layer is directly fed as input to the next. nn.Sequential is suitable for straightforward networks without complex control flow.\n",
    "# In essence, nn.Module is a general-purpose building block for creating neural networks, while nn.Sequential is a specialized container for simpler,\n",
    "# sequential architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0676af0e-f6ea-4103-b86e-bb5a5f874348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.  How can you monitor training progress in TensorFlow 2.0?\n",
    "#Ans. In TensorFlow 2.0, you can monitor training progress using tools like TensorBoard, which provides visualization of metrics, and by integrating\n",
    "#     progress bars during the training loop or using external monitoring tools like Neptune or Aporia. You can also use callbacks to record metrics and\n",
    "#     visualize training progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75059e18-3127-4145-a785-df67df73bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19.  How does the Keras API fit into TensorFlow 2.0?\n",
    "#Ans. In TensorFlow 2.0, Keras is the official high-level API for deep learning development, integrated directly into the TensorFlow library. This\n",
    "#     integration means developers can use Keras's intuitive interface for building and training models while leveraging the power and scalability of \n",
    "#     TensorFlow's backend. Essentially, Keras provides a user-friendly layer on top of TensorFlow, simplifying common deep learning tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8488d7-cac0-4cdf-a236-7f5e879e5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20.  What is an example of a deep learning project that can be implemented using TensorFlow 2.0?\n",
    "#Ans. A good example of a deep learning project using TensorFlow 2.0 is object detection, specifically using a model like YOLOv4 or Faster R-CNN. This\n",
    "#     project involves building a system that can identify and locate objects within an image or video, allowing for real-time applications like robotic\n",
    "#     harvesting or human activity detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d33f4e-5d3e-48c2-92bb-784f9cd7bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21.  What is the main advantage of using pre-trained models in TensorFlow and PyTorch?\n",
    "#Ans. The main advantage of using pre-trained models in TensorFlow and PyTorch, within the context of deep learning, is that they significantly reduce \n",
    "#     the amount of data, computation, and time required to train a model for a specific task. This is achieved through transfer learning, where\n",
    "#     knowledge gained from training on a large dataset is applied to a new, often smaller, dataset.\n",
    "#     Pre-trained models, having already learned general features and patterns from extensive data, can be fine-tuned for specific tasks with much less \n",
    "#     data and computational resources compared to training a model from scratch. This is especially beneficial when dealing with limited datasets or\n",
    "#     complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcebb5-f37c-4e28-ba37-53f59841e180",
   "metadata": {},
   "source": [
    "# Practical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60e155ed-7fa5-4da6-a35b-f10ee286c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.  How do you install and verify that TensorFlow 2.0 was installed successfully?\n",
    "#Ans. To install and verify TensorFlow 2.0 for deep learning, you'll use pip and then run a simple code snippet to confirm it's working correctly.\n",
    "#Installation:\n",
    "#Verify Pip: Ensure you have a recent version of pip. Use pip install --upgrade pip.\n",
    "#Install TensorFlow: Use pip install tensorflow for CPU or pip install tensorflow[and-cuda] for GPU support. \n",
    "\n",
    "#Verification:\n",
    "#1. Basic CPU Check:\n",
    "#Open a Python interpreter and run: import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000]))).\n",
    "#2. Basic GPU Check (Optional):\n",
    "#If you have a GPU and expect to use it, run: import tensorflow as tf; print(tf.config.list_physical_devices('GPU')). A list of GPU devices should be \n",
    "#printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfc13097-3d0f-44c4-bf10-8de0f87b251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.  How can you define a simple function in TensorFlow 2.0 to perform addition?\n",
    "#Ans. The tf.add() function returns the addition of two tf.Tensor objects element wise. The tf.Tensor object represents the multidimensional array of \n",
    "#     numbers.\n",
    "\n",
    "#Syntax:\n",
    "#tf.add( a, b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "010fb5ec-4a9c-401b-a8b2-8a6a0eeebbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  How can you create a simple neural network in TensorFlow 2.0 with one hidden layer?\n",
    "#Ans. To create a simple neural network with one hidden layer in TensorFlow 2.0, you'll use the Sequential API to define a model with an input layer,\n",
    "#     a hidden layer, and an output layer. First, import the necessary libraries like TensorFlow and Keras. Then, define the model using Sequential,\n",
    "#  adding a Dense layer for the hidden layer and another Dense layer for the output layer. Finally, compile the model and train it using the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "492c4c13-8a36-4bb9-a2a4-cac1eb7243b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.  How can you visualize the training progress using TensorFlow and Matplotlib?\n",
    "#Ans. Visualizing training progress in deep learning with TensorFlow and Matplotlib involves tracking metrics like loss and accuracy across epochs and\n",
    "#     plotting them. The fit method in TensorFlow's Keras API returns a History object containing these metrics. Matplotlib then plots this data for \n",
    "#     visualization.\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "\n",
    "# Define and compile the model\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Dense(10, activation='relu', input_shape=(784,)),\n",
    "#   tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "#])\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Load and preprocess data (example using dummy data)\n",
    "#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "#x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "#x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "#y_train = y_train[:60000]\n",
    "#y_test = y_test[:10000]\n",
    "\n",
    "# Train the model and store the history\n",
    "#history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot training and validation loss\n",
    "#plt.figure(figsize=(12, 4))\n",
    "#plt.subplot(1, 2, 1)\n",
    "#plt.plot(history.history['loss'], label='Training Loss')\n",
    "#plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('Loss')\n",
    "#plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "#plt.subplot(1, 2, 2)\n",
    "#plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "#plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.show()\n",
    "# This code snippet trains a simple neural network on the MNIST dataset and visualizes the training and validation loss and accuracy using Matplotlib.\n",
    "# The plots help in understanding how well the model is learning and whether it's overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22225435-3d51-4f4f-a93e-0d87b4b5d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.  How do you install PyTorch and verify the PyTorch installation?\n",
    "#Ans. To install PyTorch and verify its successful installation, follow these steps:\n",
    "#1. Check Prerequisites: Ensure you have Python and pip installed on your system.\n",
    "#2. Create a Virtual Environment (Recommended): Create a virtual environment to isolate PyTorch dependencies.\n",
    "#3. Select Installation Method: Navigate to the official PyTorch website and choose your preferred installation method (e.g., pip, conda).\n",
    "#4. Run Installation Command: Execute the generated installation command in your terminal, which should look like pip install torch torchvision\n",
    "#   torchaudio.\n",
    "#5. Verify Installation: After installation, verify PyTorch by running a simple code snippet like import torch; print(torch.__version__). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30c4fffa-3203-4fab-b504-6cbe5ed0518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.  How do you create a simple neural network in PyTorch?\n",
    "#Ans. To create a simple neural network in PyTorch, you'll define a class that inherits from torch.nn.Module, define the network's layers in the\n",
    "#    __init__ function, and specify how data flows through the network in the forward function. You'll also need to define a loss function, an optimizer,\n",
    "#    and train the network by iterating over your data, calculating the loss, and updating the network's parameters. \n",
    "# 1. Import Necessary Libraries:\n",
    "# import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "\n",
    "#PyTorch provides the necessary modules. \n",
    "\n",
    "#2. Define the Neural Network Architecture: \n",
    "#class SimpleNN(nn.Module):\n",
    "#    def __init__(self, input_size, hidden_size, output_size):\n",
    "#        super(SimpleNN, self).__init__()\n",
    "#        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer\n",
    "#        self.relu = nn.ReLU()                      # ReLU activation function\n",
    "#        self.fc2 = nn.Linear(hidden_size, output_size) # Another fully connected layer\n",
    "        # Optionally, you could also add a softmax for classification outputs\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "#    def forward(self, x):\n",
    "#        out = self.fc1(x)\n",
    "#        out = self.relu(out)\n",
    "#        out = self.fc2(out)\n",
    "#        # out = self.softmax(out) # Optional softmax\n",
    "#        return out\n",
    "\n",
    "# 3. Define Loss Function and Optimizer:\n",
    "# # Assuming you have a training dataset (X_train, y_train) and a test dataset (X_test, y_test)\n",
    "# and that y_train is a target tensor\n",
    "\n",
    "#input_size = X_train.shape[1]  # Number of features in your input\n",
    "#hidden_size = 64             # You can experiment with different values\n",
    "#output_size = y_train.shape[1] if isinstance(y_train, torch.Tensor) else y_train.max() + 1\n",
    "\n",
    "#model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "#criterion = nn.MSELoss()    # For regression\n",
    "# Or use nn.CrossEntropyLoss() for classification (if you are using softmax)\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer\n",
    "\n",
    "#4. Train the Network:\n",
    "#num_epochs = 10  # You can experiment with different numbers of epochs\n",
    "#for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "#    outputs = model(X_train)\n",
    "#    loss = criterion(outputs, y_train)  # Calculate the loss\n",
    "\n",
    "    # Backward and optimize\n",
    "#    optimizer.zero_grad()\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n",
    "    \n",
    "#    if (epoch+1) % 2 == 0:\n",
    "#        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "#5. Evaluate the Model (Optional):\n",
    "#with torch.no_grad(): # Disable gradient calculation for inference\n",
    "#    test_outputs = model(X_test)\n",
    "#    test_loss = criterion(test_outputs, y_test)\n",
    "#    print(f\"Test Loss: {test_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "735d3db6-d314-4502-b806-dfcd3f034cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.  How do you define a loss function and optimizer in PyTorch?\n",
    "#Ans. In the context of deep learning with PyTorch, a loss function and an optimizer are essential components for training neural networks.\n",
    "#Loss Function\n",
    "#A loss function, also known as a cost function or objective function, quantifies the discrepancy between the predicted output of a model and the actual\n",
    "#target values. It provides a measure of how well the model is performing on the training data. The goal of training is to minimize this loss function.\n",
    "#import torch.nn as nn\n",
    "\n",
    "# Example: Mean Squared Error loss for regression\n",
    "#loss_fn = nn.MSELoss()\n",
    "\n",
    "# Example: Cross-entropy loss for multiclass classification\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#Optimizer\n",
    "#An optimizer is an algorithm used to adjust the model's parameters (weights and biases) during training to minimize the loss function. It determines\n",
    "#how the model learns from the data by updating the parameters based on the gradients of the loss function.\n",
    "#PyTorch offers a variety of optimization algorithms in the torch.optim module: optim.SGD: Stochastic Gradient Descent, optim.Adam: Adaptive Moment \n",
    "#Estimation, optim.RMSprop: Root Mean Square Propagation, and optim.Adagrad: Adaptive Gradient Algorithm.\n",
    "#import torch.optim as optim\n",
    "\n",
    "# Assuming 'model' is your neural network model\n",
    "# Example: Stochastic Gradient Descent optimizer\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Example: Adam optimizer\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17dfe8dd-e678-4feb-828e-7fd88e0c2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.  How do you implement a custom loss function in PyTorch?\n",
    "#Ans. To implement a custom loss function in PyTorch, one can define a class that inherits from torch.nn.Module and overrides the forward method. This\n",
    "#     method calculates the loss based on the model's output and the target values.\n",
    "# import torch\n",
    "#import torch.nn as nn\n",
    "\n",
    "#class CustomLoss(nn.Module):\n",
    "#    def __init__(self, weight=1.0):\n",
    "#        super().__init__()\n",
    "#        self.weight = weight\n",
    "\n",
    "#    def forward(self, output, target):\n",
    "       # Custom loss calculation logic here\n",
    "#        loss = torch.mean((output - target)**2)  # Example: Mean Squared Error\n",
    "#        return loss * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "064b8d16-ff83-4d72-a001-c80195cca636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.  How do you save and load a TensorFlow model?\n",
    "#Ans. Saving a TensorFlow Model:\n",
    "#. Using model.save():\n",
    "#This function saves the entire model, including architecture, weights, and optimizer state, in a single file or directory.\n",
    "#model.save('my_model.h5') saves the model to an H5 file.\n",
    "#model.save('my_model') saves the model to a directory using the TensorFlow SavedModel format (recommended).\n",
    " \n",
    "#Loading a TensorFlow Model:\n",
    "#. Using keras.models.load_model():\n",
    "#This function loads the entire model from a saved file or directory.\n",
    "#loaded_model = keras.models.load_model('my_model.h5') loads the model from an H5 file.\n",
    "#loaded_model = keras.models.load_model('my_model') loads the model from a TensorFlow SavedModel directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec725e15-4e36-407c-b6b9-23dbc41ffb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
