{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e1d63e-489d-45ad-8525-22241ca1f6a5",
   "metadata": {},
   "source": [
    "## Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d3abd77-386e-4d59-ad04-47d5829ceb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.What is a Parameter?\n",
    "#ans. A parameter refers to a variable that is a part of the machine learning model itself,learned during training.These parameters are\n",
    " #    internal to the model and are adjusted by the learning algorithm to make predictions. They are distinct from hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "091979cb-a6e3-403b-a857-1802a8abd7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.What is correlation?What does negative correlation mean?\n",
    "#Ans. In feature enginnering, Correlation refers to the statistical measure of the relationship between two or more variables.\n",
    "#     it quantifies how strongly and in what direction two features are related, typically within a dataset. \n",
    "#     A negative correlation between two features means that as one feature increases,the other trends to be decrease,and vice versa.\n",
    "#     This indicates an inverse relationship between the features.A negative correlation is represented by a correlation coefficient \n",
    "#     value between -1 and 0 ,where -1 signifies a perfect negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af37fe7e-5873-445a-9e62-82defe8783ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Define machine learning.What are the main components in machine learning?\n",
    "#Ans. Machine learning is a field of artificial intelligence that focuses on enabling computers to learn from data and make predictions \n",
    "#     or decisions without being explicitly programmed.\n",
    "#     The main components of machine learning are data ,algorithms ,models ,and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "814d7c83-9dfb-4009-971f-bfad0f88a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.How does the loss value help in determining whether the model is good or not?\n",
    "#Ans.Loss value acts as a performance indicator in machine learning,reflecting how well a model is learning.A lower loss value generally \n",
    "#    signifies a model  that is making more accurate predictions and better at capturing the underlying patterns in the data ,while a \n",
    "#    higher loss value indicates that the model is making more errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e855e9e9-5821-4ca3-a0cc-18b01c98f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.What are continuous and categorical variables?\n",
    "#Ans. Continuous Variables\n",
    "# These are numeric variables that can take an infinite number of values within a range.\n",
    "\n",
    "#Characteristics:\n",
    "#Can be measured on a scale.\n",
    "#Have an inherent order and fixed distance between values.\n",
    "#Support mathematical operations like addition, subtraction, mean, etc.\n",
    "\n",
    "# Categorical Variables\n",
    "# These are variables that represent discrete categories or groups, often with a finite number of distinct values.\n",
    "\n",
    "#Characteristics:\n",
    "#Represent labels or groups\n",
    "#Can be nominal (no intrinsic order, like color) or ordinal (with order, like education level)\n",
    "#Cannot be used directly in most models; need encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbebb20b-595e-4d91-9126-dd8c355dd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. How do we handle categorical variables in machine learning? what are the common techniques?\n",
    "#Ans. Categorical variables in machine learning are handled through encoding techniques that convert them into a numerical format \n",
    "#     suitable for model analysis. Common methods include ordinal encoding, label encoding, one-hot encoding, binary encoding, and \n",
    "#       target encoding, each with its strengths and weaknesses. \n",
    "\n",
    "#Elaboration:\n",
    "\n",
    "#Ordinal Encoding:\n",
    "#Assigns numerical values based on the inherent order of categories. This is useful for variables like \"low,\" \"medium,\" \"high\" where the\n",
    "#ranking is meaningful. \n",
    "\n",
    "#Label Encoding:\n",
    "#Converts each category into a unique integer, which can be useful for some algorithms like decision trees but can be misleading for \n",
    "#algorithms that treat numerical inputs as continuous. \n",
    "\n",
    "#One-Hot Encoding:\n",
    "#Creates separate binary columns (0 or 1) for each category, representing the presence or absence of that category in a particular \n",
    "# instance. It's a common choice for nominal variables (no inherent order) and can be computationally intensive for categories with high\n",
    "# cardinality. \n",
    "\n",
    "#Binary Encoding:\n",
    "#Reduces the number of columns required by one-hot encoding by representing each category with a binary code, which can be more memory\n",
    "#-efficient for high-cardinality variables. \n",
    "\n",
    "#Target Encoding:\n",
    "#Replaces each category with the mean target value for that category, potentially capturing predictive relationships between categories\n",
    "#and the target variable. \n",
    "\n",
    "#Frequency Encoding:\n",
    "#Replaces categories with their frequency of occurrence, which can be helpful if category frequency is a predictor. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fa57a3c-cebe-4cbd-b8b5-8c682631c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. What do you mean by training and testing a dataset?\n",
    "#Ans. In machine learning, \"training\" a dataset refers to using a portion of the data to teach a model the patterns and relationships \n",
    "#     it needs to learn to make predictions. \"Testing\" a dataset means evaluating the trained model's performance on a separate, unseen\n",
    "#     portion of the data to gauge its ability to generalize and make accurate predictions on new, unknown data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37bf143c-7b30-402d-b8cd-77ef1de329e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. What is sklearn.preprocessing?\n",
    "#Ans. sklearn.preprocessing is a module in the scikit-learn library in Python that provides functions and classes to transform raw data \n",
    "#     into a format suitable for machine learning models. It encompasses various techniques for data scaling, normalization, encoding,\n",
    "#     and imputation, which are crucial steps in preparing data for effective model training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "687e87d8-8079-4129-a80a-be132f02a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. What is test set?\n",
    "#Ans.  a test set is a separate portion of the dataset that is not used during model training or hyperparameter tuning. It is used to \n",
    "#      evaluate the model's performance on unseen data after it has been fully trained. The test set helps assess how well the model\n",
    "#      generalizes and predicts accurately on new data, simulating real-world scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2cc1b91-57be-4f5b-883a-a5f7ef5f9496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. How do we split data for model fitting(training and testing) in python? How do we approach a machine learning problem?\n",
    "#Ans. In Python, data splitting for model fitting involves dividing your dataset into training and testing sets. The training set is \n",
    "#     used to train the machine learning model, while the testing set evaluates its performance on unseen data. A typical split is \n",
    "#     around 80% for training and 20% for testing, but this can be adjusted. Here's how to approach a machine learning problem, including\n",
    "#     data splitting: \n",
    "#1. Define the Problem: \n",
    "#   Clearly state the goal of your machine learning project. What problem are you trying to solve or what prediction are you trying to \n",
    "#   make?\n",
    "#   Understand the type of problem (classification, regression, clustering) and the relevant metrics to evaluate performance.\n",
    "#2. Data Collection: \n",
    "#  Gather the necessary data for your project. This might involve scraping data from web sources, importing data from databases, or using\n",
    "#  publicly available datasets.\n",
    "#3. Data Preparation: \n",
    "#   Cleaning: Handle missing values, outliers, and inconsistent data.\n",
    "#   Transforming: Convert categorical data into numerical representations (e.g., using one-hot encoding) and scale numerical features.\n",
    "#4. Data Splitting: \n",
    "#   Use Python libraries like scikit-learn to split your data into training and testing sets.\n",
    "#   For supervised learning, split data into features (X) and target variables (y).\n",
    "#    Example:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "    # Assuming X is your feature data and y is your target variable\n",
    "   #  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# test_size=0.2 specifies that 20% of the data will be used for testing. \n",
    "#random_state=42 ensures reproducibility by setting a seed for the random number generator. \n",
    "\n",
    "#5. Model Selection: \n",
    "# Choose an appropriate machine learning model based on the problem type and data characteristics.\n",
    "# Consider factors like model complexity, interpretability, and computational cost.\n",
    "#6. Model Training: \n",
    "# Train the selected model on the training data (X_train, y_train).\n",
    "# This involves fitting the model to the data, allowing it to learn patterns and relationships.\n",
    "#7. Model Evaluation: \n",
    "#  Evaluate the trained model's performance on the testing data (X_test, y_test).\n",
    "#  Use appropriate metrics for the problem type (e.g., accuracy, precision, recall for classification; R-squared, mean squared error for\n",
    "#  regression).\n",
    "#8. Model Tuning (Hyperparameter Optimization): \n",
    "#   Fine-tune the model's hyperparameters (e.g., learning rate, number of layers) to improve performance.\n",
    "#   Techniques include grid search, random search, or Bayesian optimization.\n",
    "#9. Prediction and Deployment: \n",
    "#   Once the model is trained and evaluated, use it to make predictions on new, unseen data.\n",
    "#   Deploy the model to a production environment for real-time prediction or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7893a7f-359c-49a4-8dd6-9248f5615fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Why do we have to perform EDA before fitting a model to the data?\n",
    "#Ans. Exploratory Data Analysis (EDA) is crucial before fitting a model because it helps uncover hidden patterns, identifies data \n",
    "#     quality issues, and provides insights into the data's characteristics, which are essential for building an accurate and reliable\n",
    "#     model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "000f292e-16de-4c9c-9ad0-321febd3979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. What is correlation?\n",
    "#Ans.  correlation refers to the statistical relationship between two or more variables, indicating how much they tend to change \n",
    "#      together. It's a way to understand the strength and direction of their linear relationship. Correlation is often represented by a \n",
    "#      correlation coefficient, which ranges from -1 to +1. Values close to 1 indicate a strong positive correlation, -1 a strong\n",
    "#      negative correlation, and 0 no linear correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b39c9140-ba7e-40cb-9779-a8b0c7fd0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. What does negative correlation mean?\n",
    "#Ans. In machine learning, a negative correlation means that as one variable increases, the other variable tends to decrease, and \n",
    "#     vice versa. This is an inverse relationship, where the variables move in opposite directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e618dae8-364e-45a9-a143-f861ab03732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C\n",
      "A  1.0  1.0 -1.0\n",
      "B  1.0  1.0 -1.0\n",
      "C -1.0 -1.0  1.0\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# 14. How can you find correlation between variables in python?\n",
    "#Ans. In Python, correlation between variables in machine learning can be found using libraries like Pandas and NumPy, which offer \n",
    "#     functions for calculating correlation coefficients like Pearson's correlation. These coefficients indicate the strength and \n",
    "#    direction of the linear relationship between variables, ranging from -1 to 1. \n",
    "#Here's a breakdown of how to find correlation in Python:\n",
    "#1. Libraries and Functions:\n",
    "#Pandas: The corr() method on DataFrames calculates pairwise correlation of all columns. \n",
    "#NumPy: The corrcoef() function calculates Pearson's correlation coefficient. \n",
    "#SciPy: Provides functions like pearsonr(), spearmanr(), and kendalltau() for different correlation measures. \n",
    "#2. Examples:\n",
    "#Using Pandas.\n",
    "import pandas as pd\n",
    "\n",
    "    # Sample DataFrame\n",
    "data = {'A': [1, 2, 3, 4, 5],\n",
    "            'B': [2, 4, 6, 8, 10],\n",
    "            'C': [5, 4, 3, 2, 1]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)\n",
    "#This code calculates the correlation between all pairs of columns in the DataFrame df. Using NumPy. \n",
    "import numpy as np\n",
    "\n",
    "    # Sample arrays\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "    # Calculate Pearson's correlation\n",
    "corr_coef = np.corrcoef(x, y)[0, 1]\n",
    "print(corr_coef)\n",
    "#This calculates Pearson's correlation coefficient between the arrays x and y. \n",
    "#3. Interpreting Correlation:\n",
    "#Positive Correlation: As one variable increases, the other tends to increase as well. A value close to 1 indicates a strong positive \n",
    "#correlation.\n",
    "#Negative Correlation: As one variable increases, the other tends to decrease. A value close to -1 indicates a strong negative \n",
    "#correlation.\n",
    "#Zero Correlation: There is no linear relationship between the variables. \n",
    "#4. Visualization:\n",
    "#Scatter Plots:\n",
    "#Visualizing data with scatter plots helps identify the relationship between two variables. \n",
    "#Heatmaps:\n",
    "#Heatmaps are useful for visualizing correlation matrices, especially when dealing with a large number of variables. \n",
    "#5. Considerations:\n",
    "#Correlation does not imply causation:\n",
    "#Even if variables are correlated, it doesn't mean that one causes the other. \n",
    "#Linear Relationships:\n",
    "#Correlation measures the strength of linear relationships. Non-linear relationships might not be captured by correlation coefficients. \n",
    "#Other Correlation Measures:\n",
    "#Besides Pearson's correlation, Spearman's and Kendall's tau correlation can be used for different types of relationships, such as\n",
    "#monotonic relationships. \n",
    "#By using these libraries and understanding the nuances of correlation, you can effectively analyze relationships between variables in \n",
    "#your machine learning projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91f82e52-f157-4d55-987b-eb19fde92699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. What is causation? Explain difference between correlation and causation with an example.\n",
    "#Ans. Causation means that one event directly leads to another, while correlation simply means that two events are related, but one \n",
    "#     doesn't necessarily cause the other. In feature engineering, understanding this distinction is crucial to avoid building models \n",
    "#     that rely on spurious relationships, which can lead to incorrect predictions. \n",
    "#Example:\n",
    "# In a marketing campaign, you might observe a correlation between spending on social media advertising (X) and sales (Y). However, this\n",
    "# doesn't mean that social media advertising causes increased sales. It's possible that both are influenced by a third factor, like a \n",
    "# broader seasonal trend or a promotional event. To establish causation, you would need to conduct a controlled experiment, where you \n",
    "# vary social media advertising spending while keeping other factors constant, and then measure the impact on sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd1edeb3-0d2d-4785-950b-b2974ace5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "#Ans. An optimizer is an algorithm that adjusts model parameters (like weights and biases) to minimize a loss function, guiding the model\n",
    "#     towards better predictions during training. Different types of optimizers, such as Gradient Descent, Stochastic Gradient Descent\n",
    "#     (SGD),and Adam, each employ different strategies to find the optimal parameters. \n",
    "#How it works: Iteratively updates model parameters in the direction of the negative gradient of the loss function, aiming to find the\n",
    "#     minimum of the loss surface. \n",
    "#Example in Feature Engineering: When training a linear regression model to predict house prices based on features like square footage,\n",
    "#    number of bedrooms, and location, Gradient Descent can be used to find the optimal weights for these features. \n",
    "#2. Stochastic Gradient Descent (SGD):\n",
    "#How it works: An extension of Gradient Descent that updates model parameters based on the gradient of a single training example (or a \n",
    "#   small batch) instead of the entire dataset. \n",
    "#Example in Feature Engineering: In a high-dimensional classification problem (e.g., predicting customer churn based on many features),\n",
    "#   SGD can be used to efficiently train a logistic regression model by processing data in smaller batches. \n",
    "#3. Adam (Adaptive Moment Estimation):\n",
    "#How it works: Combines the benefits of momentum-based methods (like SGD with momentum) and adaptive learning rate methods, providing \n",
    "#   efficient and stable convergence. \n",
    "#Example in Feature Engineering: When training a complex deep learning model to perform image recognition, Adam can be used to \n",
    "#   efficiently find the optimal parameters for the model's layers and features. \n",
    "#4. Adagrad (Adaptive Gradient Descent):\n",
    "#How it works: Adapts the learning rate for each parameter based on the historical gradients, providing faster convergence for sparse\n",
    "#   data. \n",
    "#Example in Feature Engineering: In text classification tasks, where some words might appear infrequently, Adagrad can be useful for\n",
    "#   adjusting the learning rate for each word's corresponding weight. \n",
    "#5. RMSprop (Root Mean Square Propagation):\n",
    "# How it works:Uses a running average of squared gradients to adjust the learning rate, providing stability and preventing oscillations. \n",
    "# Example in Feature Engineering: When training a recurrent neural network (RNN) for time series forecasting, RMSprop can be used to help\n",
    "#   stabilize the training process and prevent the model from overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17c831bb-eac6-44aa-8717-b5f5e7c17654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. What is sklearn.linear_model ?\n",
    "#Ans. sklearn.linear_model is a module within the Scikit-learn library that provides various linear models and algorithms for regression\n",
    "#     and classification tasks. It offers tools for creating, training, and using these models, ultimately helping to improve model \n",
    "#     performance and accuracy by intelligently transforming or selecting features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc42166a-a23f-4fdf-84f1-c8a2e0100c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. What does model.fit() do? What arguments must be given?\n",
    "#Ans. In the context of machine learning and particularly feature engineering, model.fit() is the method used to train a model on a given\n",
    "#     dataset. It takes the input data and the target variable (if applicable) and adjusts the model's internal parameters (like weights\n",
    "#     and biases) to minimize a loss function, essentially teaching the model to make accurate predictions. \n",
    "#Arguments:\n",
    "#1. X (Input Data):\n",
    "#  This is a matrix or array representing the feature data. In feature engineering, X will contain the transformed features you've \n",
    "#  created,such as scaled values, one-hot encoded categories, or new features derived from existing ones. \n",
    "#2. y (Target Variable):\n",
    "#  For supervised learning models, y is the target variable or the variable you are trying to predict. This is the variable that the\n",
    "#  model will learn to predict based on the input features (the X matrix). \n",
    "#3. sample_weight (Optional):\n",
    "#  In some cases, you might want to assign different weights to different samples in your training data. This can be helpful if some \n",
    "#  samples are more informative than others or if you need to account for class imbalances. \n",
    "#4. fit_intercept (Optional, for Linear Models):\n",
    "#  This argument specifies whether the model should include an intercept term in its predictions. For linear regression models, this is\n",
    "#  a parameter that determines the y-intercept of the regression line. \n",
    " \n",
    "#In essence, model.fit() is the engine that drives model training in machine learning. It's where the model learns from the data and \n",
    "#adjusts its parameters to minimize errors. When feature engineering is involved, the data passed to model.fit() is the result of \n",
    "#transforming the raw data to create more informative and useful features for the model to learn from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99abba96-989a-4173-9cc8-e7728ba5b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. What does model.predict() do? What arguments must be given?\n",
    "#Ans. In the context of machine learning, model.predict() is a method used to make predictions on new data using a trained model. It \n",
    "#     takes one argument: the input data on which predictions are to be made. \n",
    "#Argument:\n",
    "#The primary argument required by model.predict() is the design matrix (often represented as X) containing the features of the data for \n",
    "#which predictions are needed. This design matrix should have the same number of columns (features) as the one used during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9560c429-f35f-4c7e-922e-7cb738a37b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. What are continuous and categorical variables?\n",
    "#Ans. In feature engineering, continuous variables represent numerical data that can take any value within a range, while categorical\n",
    "#     variables represent non-numerical data that can be grouped into distinct categories. Continuous variables are typically used for\n",
    "#     modeling and analysis when their values are naturally continuous, while categorical variables often require encoding to be used in\n",
    "#     machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d89bb9a2-80fe-43c5-b159-5da41b42e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. What is feature scaling? How does it help in Machine Learning?\n",
    "#Ans. Feature scaling, also known as normalization or standardization, is a preprocessing technique in machine learning that transforms\n",
    "#     the numerical features of a dataset to a common scale or range. This is crucial for many algorithms, especially those based on \n",
    "#     distance or gradient calculations (like k-nearest neighbors, support vector machines, and neural networks). By ensuring features \n",
    "#     are on a similar scale, feature scaling helps prevent features with larger ranges from dominating the learning process and improves\n",
    "#     model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2525d391-2bbb-47a3-abf8-91f85d578ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. How do we perform scaling in Python?\n",
    "#Ans.  Common scaling methods that can be performed include:\n",
    "#   >. Min-Max Scaling (Normalization): Scales data to a range between 0 and 1.\n",
    "#    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "#    scaler = MinMaxScaler()\n",
    "#    scaled_data = scaler.fit_transform(data)\n",
    "#   >. Standardization (Z-score normalization): Scales data to have a mean of 0 and a standard deviation of 1.\n",
    "#    from sklearn.preprocessing import StandardScaler\n",
    "#    \n",
    "#    scaler = StandardScaler()\n",
    "#    scaled_data = scaler.fit_transform(data)\n",
    "#   >. MaxAbs Scaling: Scales data to the range [-1, 1] by dividing by the maximum absolute value.\n",
    "  #  from sklearn.preprocessing import MaxAbsScaler\n",
    "    \n",
    "  #  scaler = MaxAbsScaler()\n",
    "  #  scaled_data = scaler.fit_transform(data)\n",
    "# Robust Scaling: Scales data using statistics that are robust to outliers.\n",
    "#     from sklearn.preprocessing import RobustScaler\n",
    "    \n",
    "   # scaler = RobustScaler()\n",
    "   # scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Unit Vector Scaling: Scales the norm of each sample to 1.\n",
    "#     from sklearn.preprocessing import normalize\n",
    "     #  scaled_data = normalize(data)\n",
    "# These methods are implemented using scikit-learn's preprocessing module. The choice of scaling method depends on the data distribution\n",
    "# and the specific requirements of the machine learning algorithm being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9783189c-9c31-4538-bc20-0f94c1bb3d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. What is sklearn.preprocessing?\n",
    "#Ans. sklearn.preprocessing is a module in the scikit-learn library in Python that provides functions and classes to preprocess data \n",
    "#     before training machine learning models. Preprocessing is a crucial step in machine learning as it transforms raw data into a \n",
    "#     suitable format for algorithms, improving model performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b31c44ea-414b-49c0-8ed6-b2859d64e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. How do we split data for model fitting (training and testing) in Python?\n",
    "#Ans.In Python, data is typically split into training and testing sets using the train_test_split function from the scikit-learn library.\n",
    "#    This function randomly divides the data into two subsets: one for training the model and another for evaluating its performance on \n",
    "#    unseen data. \n",
    "#Here's how to use train_test_split:\n",
    "# 1.Import the function:\n",
    "  # from sklearn.model_selection import train_test_split\n",
    "# 2.Separate your data:\n",
    "   #Before splitting, separate your data into features (X) and target (y). \n",
    "   #If you have a Pandas DataFrame, you can use .drop() to create the features (X) by dropping the target column and assign the target\n",
    "   #column to y: \n",
    "    # import pandas as pd\n",
    "     # Assuming your data is in a Pandas DataFrame called 'df'\n",
    "     # and the target column is named 'target'\n",
    "    # X = df.drop('target', axis=1)\n",
    "     #y = df['target']\n",
    "# 3.Call train_test_split:\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X and y are your input data. \n",
    "#test_size=0.2 specifies that 20% of the data will be allocated to the test set. The remaining 80% will be used for training. \n",
    "#random_state=42 is used for reproducibility. If you remove this or set it to None, the split will be random each time you run the code. \n",
    "# 4.Verify the split:\n",
    "#   print(\"Shape of X_train:\", X_train.shape)\n",
    "#   print(\"Shape of X_test:\", X_test.shape)\n",
    "#   print(\"Shape of y_train:\", y_train.shape)\n",
    "#   print(\"Shape of y_test:\", y_test.shape)\n",
    "# 5. Train your model:\n",
    "#Use the X_train and y_train data to train your chosen machine learning model. \n",
    "# 6. Test your model:\n",
    "#Use the X_test and y_test data to evaluate how well your trained model performs on unseen data. Make predictions using X_test and \n",
    "#compare these predictions to the actual values in y_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8553ee9-69ac-468e-a467-4322685b3e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. Explain data encoding?\n",
    "#Ans. In machine learning, data encoding refers to the process of converting data from one format to another, typically to transform \n",
    "#     categorical or text data into a numerical format that machine learning algorithms can understand and process. This conversion is\n",
    "#     crucial because most machine learning models require numerical input. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
